{
  "$schema": "https://ui.shadcn.com/schema/registry-item.json",
  "name": "speech-input",
  "type": "registry:component",
  "title": "Speech Input",
  "description": "AI-powered speech input component.",
  "files": [
    {
      "path": "registry/default/ai-elements/speech-input.tsx",
      "type": "registry:component",
      "content": "\"use client\";\n\nimport { Button } from \"@/components/ui/button\";\nimport { cn } from \"@/lib/utils\";\nimport { LoaderIcon, MicIcon, SquareIcon } from \"lucide-react\";\nimport {\n  type ComponentProps,\n  useCallback,\n  useEffect,\n  useRef,\n  useState,\n} from \"react\";\n\ninterface SpeechRecognition extends EventTarget {\n  continuous: boolean;\n  interimResults: boolean;\n  lang: string;\n  start(): void;\n  stop(): void;\n  onstart: ((this: SpeechRecognition, ev: Event) => any) | null;\n  onend: ((this: SpeechRecognition, ev: Event) => any) | null;\n  onresult:\n    | ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any)\n    | null;\n  onerror:\n    | ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any)\n    | null;\n}\n\ninterface SpeechRecognitionEvent extends Event {\n  results: SpeechRecognitionResultList;\n  resultIndex: number;\n}\n\ntype SpeechRecognitionResultList = {\n  readonly length: number;\n  item(index: number): SpeechRecognitionResult;\n  [index: number]: SpeechRecognitionResult;\n};\n\ntype SpeechRecognitionResult = {\n  readonly length: number;\n  item(index: number): SpeechRecognitionAlternative;\n  [index: number]: SpeechRecognitionAlternative;\n  isFinal: boolean;\n};\n\ntype SpeechRecognitionAlternative = {\n  transcript: string;\n  confidence: number;\n};\n\ninterface SpeechRecognitionErrorEvent extends Event {\n  error: string;\n}\n\ndeclare global {\n  // biome-ignore lint/style/useConsistentTypeDefinitions: We need to declare the SpeechRecognition interface\n  interface Window {\n    SpeechRecognition: {\n      new (): SpeechRecognition;\n    };\n    webkitSpeechRecognition: {\n      new (): SpeechRecognition;\n    };\n  }\n}\n\ntype SpeechInputMode = \"speech-recognition\" | \"media-recorder\" | \"none\";\n\nexport type SpeechInputProps = ComponentProps<typeof Button> & {\n  onTranscriptionChange?: (text: string) => void;\n  /**\n   * Callback for when audio is recorded using MediaRecorder fallback.\n   * This is called in browsers that don't support the Web Speech API (Firefox, Safari).\n   * The callback receives an audio Blob that should be sent to a transcription service.\n   * Return the transcribed text, which will be passed to onTranscriptionChange.\n   */\n  onAudioRecorded?: (audioBlob: Blob) => Promise<string>;\n  lang?: string;\n};\n\nconst detectSpeechInputMode = (): SpeechInputMode => {\n  if (typeof window === \"undefined\") {\n    return \"none\";\n  }\n\n  if (\"SpeechRecognition\" in window || \"webkitSpeechRecognition\" in window) {\n    return \"speech-recognition\";\n  }\n\n  if (\"MediaRecorder\" in window && \"mediaDevices\" in navigator) {\n    return \"media-recorder\";\n  }\n\n  return \"none\";\n};\n\nexport const SpeechInput = ({\n  className,\n  onTranscriptionChange,\n  onAudioRecorded,\n  lang = \"en-US\",\n  ...props\n}: SpeechInputProps) => {\n  const [isListening, setIsListening] = useState(false);\n  const [isProcessing, setIsProcessing] = useState(false);\n  const [mode, setMode] = useState<SpeechInputMode>(\"none\");\n  const [recognition, setRecognition] = useState<SpeechRecognition | null>(\n    null\n  );\n  const recognitionRef = useRef<SpeechRecognition | null>(null);\n  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n  const audioChunksRef = useRef<Blob[]>([]);\n\n  // Detect mode on mount\n  useEffect(() => {\n    setMode(detectSpeechInputMode());\n  }, []);\n\n  // Initialize Speech Recognition when mode is speech-recognition\n  useEffect(() => {\n    if (mode !== \"speech-recognition\") {\n      return;\n    }\n\n    const SpeechRecognition =\n      window.SpeechRecognition || window.webkitSpeechRecognition;\n    const speechRecognition = new SpeechRecognition();\n\n    speechRecognition.continuous = true;\n    speechRecognition.interimResults = true;\n    speechRecognition.lang = lang;\n\n    speechRecognition.onstart = () => {\n      setIsListening(true);\n    };\n\n    speechRecognition.onend = () => {\n      setIsListening(false);\n    };\n\n    speechRecognition.onresult = (event) => {\n      let finalTranscript = \"\";\n\n      for (let i = event.resultIndex; i < event.results.length; i++) {\n        const result = event.results[i];\n        if (result.isFinal) {\n          finalTranscript += result[0]?.transcript ?? \"\";\n        }\n      }\n\n      if (finalTranscript) {\n        onTranscriptionChange?.(finalTranscript);\n      }\n    };\n\n    speechRecognition.onerror = (event) => {\n      console.error(\"Speech recognition error:\", event.error);\n      setIsListening(false);\n    };\n\n    recognitionRef.current = speechRecognition;\n    setRecognition(speechRecognition);\n\n    return () => {\n      if (recognitionRef.current) {\n        recognitionRef.current.stop();\n      }\n    };\n  }, [mode, onTranscriptionChange, lang]);\n\n  // Start MediaRecorder recording\n  const startMediaRecorder = useCallback(async () => {\n    if (!onAudioRecorded) {\n      console.warn(\n        \"SpeechInput: onAudioRecorded callback is required for MediaRecorder fallback\"\n      );\n      return;\n    }\n\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n      const mediaRecorder = new MediaRecorder(stream);\n      audioChunksRef.current = [];\n\n      mediaRecorder.ondataavailable = (event) => {\n        if (event.data.size > 0) {\n          audioChunksRef.current.push(event.data);\n        }\n      };\n\n      mediaRecorder.onstop = async () => {\n        // Stop all tracks to release the microphone\n        for (const track of stream.getTracks()) {\n          track.stop();\n        }\n\n        const audioBlob = new Blob(audioChunksRef.current, {\n          type: \"audio/webm\",\n        });\n\n        if (audioBlob.size > 0) {\n          setIsProcessing(true);\n          try {\n            const transcript = await onAudioRecorded(audioBlob);\n            if (transcript) {\n              onTranscriptionChange?.(transcript);\n            }\n          } catch (error) {\n            console.error(\"Transcription error:\", error);\n          } finally {\n            setIsProcessing(false);\n          }\n        }\n      };\n\n      mediaRecorder.onerror = (event) => {\n        console.error(\"MediaRecorder error:\", event);\n        setIsListening(false);\n        // Stop all tracks on error\n        for (const track of stream.getTracks()) {\n          track.stop();\n        }\n      };\n\n      mediaRecorderRef.current = mediaRecorder;\n      mediaRecorder.start();\n      setIsListening(true);\n    } catch (error) {\n      console.error(\"Failed to start MediaRecorder:\", error);\n      setIsListening(false);\n    }\n  }, [onAudioRecorded, onTranscriptionChange]);\n\n  // Stop MediaRecorder recording\n  const stopMediaRecorder = useCallback(() => {\n    if (mediaRecorderRef.current?.state === \"recording\") {\n      mediaRecorderRef.current.stop();\n    }\n    setIsListening(false);\n  }, []);\n\n  const toggleListening = useCallback(() => {\n    if (mode === \"speech-recognition\" && recognition) {\n      if (isListening) {\n        recognition.stop();\n      } else {\n        recognition.start();\n      }\n    } else if (mode === \"media-recorder\") {\n      if (isListening) {\n        stopMediaRecorder();\n      } else {\n        startMediaRecorder();\n      }\n    }\n  }, [\n    mode,\n    recognition,\n    isListening,\n    startMediaRecorder,\n    stopMediaRecorder,\n  ]);\n\n  // Determine if button should be disabled\n  const isDisabled =\n    mode === \"none\" ||\n    (mode === \"speech-recognition\" && !recognition) ||\n    (mode === \"media-recorder\" && !onAudioRecorded) ||\n    isProcessing;\n\n  return (\n    <div className=\"relative inline-flex items-center justify-center\">\n      {/* Animated pulse rings */}\n      {isListening &&\n        [0, 1, 2].map((index) => (\n          <div\n            className=\"absolute inset-0 animate-ping rounded-full border-2 border-red-400/30\"\n            key={index}\n            style={{\n              animationDelay: `${index * 0.3}s`,\n              animationDuration: \"2s\",\n            }}\n          />\n        ))}\n\n      {/* Main record button */}\n      <Button\n        className={cn(\n          \"relative z-10 rounded-full transition-all duration-300\",\n          isListening\n            ? \"bg-destructive text-white hover:bg-destructive/80 hover:text-white\"\n            : \"bg-primary text-primary-foreground hover:bg-primary/80 hover:text-primary-foreground\",\n          className\n        )}\n        disabled={isDisabled}\n        onClick={toggleListening}\n        {...props}\n      >\n        {isProcessing ? (\n          <LoaderIcon className=\"size-4 animate-spin\" />\n        ) : isListening ? (\n          <SquareIcon className=\"size-4\" />\n        ) : (\n          <MicIcon className=\"size-4\" />\n        )}\n      </Button>\n    </div>\n  );\n};\n",
      "target": "components/ai-elements/speech-input.tsx"
    }
  ],
  "dependencies": [
    "lucide-react"
  ],
  "devDependencies": [],
  "registryDependencies": [
    "button"
  ]
}